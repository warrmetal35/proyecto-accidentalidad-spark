# Sistema de AnÃ¡lisis de Accidentalidad con Apache Spark y Kafka

## ğŸ¯ DescripciÃ³n del Proyecto
Este proyecto implementa una soluciÃ³n completa de anÃ¡lisis de datos para el monitoreo y estudio de accidentalidad vial. Combina tÃ©cnicas de procesamiento batch para anÃ¡lisis histÃ³rico y streaming en tiempo real para la detecciÃ³n inmediata de patrones y tendencias. La soluciÃ³n permite a entidades gubernamentales y organizaciones de seguridad vial tomar decisiones basadas en datos tanto histÃ³ricos como en tiempo real.

## ğŸ“Š Contexto del Problema
La accidentalidad vial representa un desafÃ­o significativo en seguridad pÃºblica y salud. Este sistema aborda la necesidad de:
- **AnÃ¡lisis histÃ³rico**: Identificar patrones estacionales, zonas de alto riesgo y factores contribuyentes en datos acumulados
- **Monitoreo en tiempo real**: Detectar incrementos sÃºbitos en accidentalidad, correlacionar eventos y generar alertas tempranas
- **VisualizaciÃ³n integral**: Proporcionar insights accionables para polÃ­ticas de prevenciÃ³n y optimizaciÃ³n de recursos de emergencia

## ğŸ—ï¸ Arquitectura del Sistema

### Diagrama de Flujo de Datos

Fuente de Datos HistÃ³ricos (CSV)
â†“
Spark Batch Processing
â†“
AnÃ¡lisis Exploratorio (EDA)
â†“
Modelos de Tendencia

Generador Simulado de Eventos
â†“
Apache Kafka (Message Broker)
â†“
Spark Streaming Processing
â†“
Dashboard Tiempo Real + Alertas

### Componentes Principales
1. **Capa de Ingesta**: Kafka como buffer de mensajes para datos en tiempo real
2. **Capa de Procesamiento**: Spark para transformaciones complejas y agregaciones
3. **Capa de Almacenamiento**: Parquet para datos limpios y resultados de anÃ¡lisis
4. **Capa de VisualizaciÃ³n**: Consola Spark UI y outputs estructurados

## ğŸ› ï¸ Stack TecnolÃ³gico Detallado

### Procesamiento de Datos
- **Apache Spark 3.5.0**: Motor de procesamiento distribuido para operaciones batch y streaming
- **Spark SQL**: Consultas estructuradas y optimizaciones Catalyst
- **Structured Streaming**: Procesamiento en tiempo real con semÃ¡ntica exactly-once

### MensajerÃ­a y Tiempo Real
- **Apache Kafka 3.7.2**: Plataforma de streaming distribuida para ingesta de eventos
- **Kafka Producers/Consumers**: Client libraries para Python y Spark integration

### Lenguajes y LibrerÃ­as
- **Python 3**: Lenguaje principal de desarrollo
- **PySpark**: API de Python para Spark
- **Pandas**: Procesamiento de datos en memoria para el generador
- **JSON**: SerializaciÃ³n de mensajes entre componentes

## ğŸ“ Estructura del Proyecto
proyecto-accidentalidad-spark/
â”‚
â”œâ”€â”€ data/
â”‚ â”œâ”€â”€ raw/accidentes_2017_2022.csv # Dataset original
â”‚ â””â”€â”€ processed/ # Resultados EDA
â”‚
â”œâ”€â”€ src/
â”‚ â”œâ”€â”€ batch/
â”‚ â”‚ â””â”€â”€ spark_batch_analysis.py # AnÃ¡lisis histÃ³rico
â”‚ â”œâ”€â”€ streaming/
â”‚ â”‚ â”œâ”€â”€ kafka_accidentes_producer.py # Generador eventos
â”‚ â”‚ â””â”€â”€ spark_streaming_consumer.py # Procesamiento real-time
â”‚ â””â”€â”€ utils/
â”‚ â””â”€â”€ config.py # Configuraciones
â”‚
â”œâ”€â”€ results/
â”‚ â”œâ”€â”€ accidentalidad_clean/ # Datos procesados
â”‚ â””â”€â”€ analysis_reports/ # Reportes generados
â”‚
â”œâ”€â”€ docs/
â”‚ â”œâ”€â”€ architecture_diagram.png
â”‚ â””â”€â”€ setup_instructions.md
â”‚
â””â”€â”€ README.md

## ğŸš€ GuÃ­a de InstalaciÃ³n y ConfiguraciÃ³n

### Prerrequisitos del Sistema

# Versiones requeridas
Java 11+, Python 3.8+, Apache Spark 3.5+, Kafka 3.7+

# InstalaciÃ³n de dependencias Python
pip install kafka-python==2.0.2
pip install pandas==1.5.3
pip install pyspark==3.5.0

ConfiguraciÃ³n de Entorno

# Variables de entorno requeridas
export SPARK_HOME=/opt/spark
export KAFKA_HOME=/opt/kafka
export PATH=$PATH:$SPARK_HOME/bin:$KAFKA_HOME/bin

ğŸ’» EjecuciÃ³n del Sistema
Fase 1: Procesamiento Batch (AnÃ¡lisis HistÃ³rico)

# Ejecutar anÃ¡lisis exploratorio sobre datos histÃ³ricos
spark-submit spark_batch_analysis.py

# Este script realiza:
 - Limpieza y transformaciÃ³n de datos crudos
 - AnÃ¡lisis de distribuciÃ³n temporal (aÃ±os, meses, horas)
 - Agregaciones por gravedad y patrones estacionales
 - GeneraciÃ³n de datasets listos para reporting

Fase 2: Procesamiento Streaming (Tiempo Real)
# Paso 2.1: Iniciar el generador de eventos de accidentalidad
python3 kafka_accidentes_producer.py

# Este componente:
 - Lee el dataset histÃ³rico y simula eventos en tiempo real
 - Publica mensajes JSON al topic de Kafka cada 5 segundos
 - Incluye metadatos completos de cada accidente simulado

# Paso 2.2: Iniciar el consumidor Spark Streaming
spark-submit --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1 spark_streaming_consumer.py

# Este componente:
 - Se suscribe al topic de Kafka y consume mensajes
 - Aplica ventanas deslizantes de 1 minuto para agregaciones
 - Calcula mÃ©tricas en tiempo real por gravedad de accidente
 - Output continuo a consola con estadÃ­sticas actualizadas

Fase 3: Monitoreo y VisualizaciÃ³n
# Acceder a la interfaz web de Spark
http://localhost:4040 (se reemplazo por mi ip)

# La UI proporciona:
 - MÃ©tricas de ejecuciÃ³n de jobs en tiempo real
 - Monitoring de recursos y performance
 - Debugging de operaciones de streaming

ğŸ“Š Resultados y AnÃ¡lisis de Datos
Hallazgos del AnÃ¡lisis Batch
El procesamiento histÃ³rico revelÃ³ patrones significativos en la accidentalidad:

DistribuciÃ³n Temporal:
Tendencia anual de accidentes por gravedad
Patrones estacionales por meses del aÃ±o
Horarios pico de incidencia de accidentes
SegmentaciÃ³n por Gravedad:
ProporciÃ³n entre accidentes con heridos vs solo daÃ±os materiales
CorrelaciÃ³n entre factores temporales y severidad
MÃ©tricas en Tiempo Real
El sistema de streaming permite:

Agregaciones Continuas:
Conteo de accidentes por ventanas de 1 minuto
DistribuciÃ³n en tiempo real por tipo de gravedad
DetecciÃ³n de picos anÃ³malos en la frecuencia

VisualizaciÃ³n Operacional:
Dashboard actualizado cada batch de procesamiento
Tracking de throughput del sistema
Latencia de procesamiento end-to-end

ğŸ–¼ï¸ Evidencias de EjecuciÃ³n

#Captura 1: Pipeline de Ingesta en Tiempo Real
(https://github.com/warrmetal35/proyecto-accidentalidad-spark/blob/main/producer_kafka.png.png)
Kafka Producer generando eventos de accidentalidad simulados cada 5 segundos

